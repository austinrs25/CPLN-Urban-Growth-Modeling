---
title: "CPLN 6750 - Urban Growth Modeling Assignment, Spring 2025"
author: "Abe Doroshow & charlotte Studner-Sutherland"
date: "5/10/2025"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }


</style>

# 1. Introduction

*Insert introduction here*

# 1.2. The Workflow

*Explain workflow here*

# 1.4. Setup - Loading Libraries and Functions

*Explain loading libraries*

```{r load_packages, message=FALSE, warning=FALSE, results = "hide"}
library(tidyverse)
library(sf)
library(raster)
library(kableExtra)
library(tidycensus)
library(tigris)
library(FNN)
library(caret)
library(yardstick)
library(plotROC) 
library(ggrepel)
library(pROC)
library(grid)
library(gridExtra)
library(viridis)
library(igraph)
library(mapview)
library(FedData)

palette2 <- c("#41b6c4","#253494")
palette4 <- c("#a1dab4","#41b6c4","#2c7fb8","#253494")
palette5 <- c("#ffffcc","#a1dab4","#41b6c4","#2c7fb8","#253494")
palette10 <- c("#f7fcf0","#e0f3db","#ccebc5","#a8ddb5","#7bccc4",
               "#4eb3d3","#2b8cbe","#0868ac","#084081","#f7fcf0")
```

*Explain loading functions and creating empty fishnet*

```{r, warning = FALSE, message = FALSE}
#this function converts a column in to quintiles. It is used for mapping.
quintileBreaks <- function(df,variable) {
    as.character(quantile(df[[variable]],
                          c(.01,.2,.4,.6,.8),na.rm=T))
}

#This function can be used to convert a polygon sf to centroids xy coords.
xyC <- function(aPolygonSF) {
  as.data.frame(
    cbind(x=st_coordinates(st_centroid(aPolygonSF))[,1],
          y=st_coordinates(st_centroid(aPolygonSF))[,2]))
} 

#this function convert a raster to a data frame so it can be plotted in ggplot
rast <- function(inRaster) {
  data.frame(
    xyFromCell(inRaster, 1:ncell(inRaster)), 
    value = getValues(inRaster)) }

# knn distance function

nn_function <- function(measureFrom,measureTo,k) {
  #convert the sf layers to matrices
  measureFrom_Matrix <-
    as.matrix(measureFrom)
  measureTo_Matrix <-
    as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
    output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull()
  
  return(output)  
}

aggregateRaster <- function(inputRasterList, theFishnet) {
  #create an empty fishnet with the same dimensions as the input fishnet
  theseFishnets <- theFishnet %>% dplyr::select()
  #for each raster in the raster list
  for (i in inputRasterList) {
  #create a variable name corresponding to the ith raster
  varName <- names(i)
  #convert raster to points as an sf
    thesePoints <-
      rasterToPoints(i) %>%
      as.data.frame() %>%
      st_as_sf(coords = c("x", "y"), crs = st_crs(theFishnet)) %>%
      filter(.[[1]] == 1)
  #aggregate to the fishnet
    thisFishnet <-
      aggregate(thesePoints, theFishnet, length) %>%
      mutate(!!varName := ifelse(is.na(.[[1]]),0,1))
  #add to the larger fishnet
    theseFishnets <- cbind(theseFishnets,thisFishnet)
  }
  #output all aggregates as one large fishnet
   return(theseFishnets)
  }
```

# 2. Loading LC and MSA data

## 2.1. Load the study area MSA

*Explain loading study area and coordinate system*

```{r}
charlotteMSA <- 
  st_read("https://raw.githubusercontent.com/austinrs25/CPLN-Urban-Growth-Modeling/refs/heads/main/charlotteMSA.geojson") 
charlotteMSA %>% st_transform(2264)

```
## 2.2. Load Land Cover for t1 and t2

*Explain loading landcover*

```{r}

lc_2011 <- get_nlcd(
    template = charlotteMSA,
    label = "lc_2011",
    year = 2011
  ) %>%
  raster(.)

```



```{r}

lc_2019 <- get_nlcd(
    template = charlotteMSA,
    label = "lc_2019",
    year = 2019
  ) %>%
  raster(.)

```

# 3. Detecting Land Cover Conversion

*Text to explain land cover conversion*

## 3.1. Resampling Rasters

*Explain resampling to reduce raster resolution*

```{r}
lc_2011_rs <- aggregate(lc_2011, fact = 30, fun = "modal")

lc_2019_rs <- aggregate(lc_2019, fact = 30, fun = "modal")

```

## 3.2. Reclassifying As Developed and Undeveloped

*Explain reclassification as developed and undeveloped*

```{r}
reclassMatrix <- 
  matrix(c(
    0,12,0,
    12,24,1,
    24,Inf,0),
  ncol=3, byrow=T)
```



```{r, warning = FALSE, message = FALSE}
developed_2011 <- 
  reclassify(lc_2011_rs, reclassMatrix)

developed_2019 <- 
  reclassify(lc_2019_rs, reclassMatrix)

```


*Map algebra explained: where did land cover change?*


```{r, warning = FALSE, message = FALSE}

development_change <- developed_2011 + developed_2019

```

*histogram shows frequency of change from undeveloped to developed between 2011 and 2019*

The 1's represent the change.

```{r, warning = FALSE, message = FALSE}
hist(development_change)
```

*remove 2's and 0's*

```{r, warning = FALSE, message = FALSE}
development_change[development_change != 1] <- NA
```

## 3.3. Examining Land Cover Change

*view land cover change raster on map*

```{r}
mapView(development_change)

```

# 4. Creating the Fishnet

*Explain fishnet creation*

```{r, warning = FALSE, message = FALSE}
charlotteMSA_fishnet <- 
  st_make_grid(charlotteMSA %>%
                     st_transform(crs(development_change)),
                 cellsize = res(development_change)[1], 
                 square = TRUE) %>% 
  st_sf() %>% 
  st_intersection(., charlotteMSA %>%
                    dplyr::select(geometry) %>%
                         st_transform(crs(development_change))) %>%
  mutate(uniqueID = rownames(.))
```

# 5. Join Raster Data to the Fishnet

*Explain joining raster data to fishnet*

## 5.1. Aggregating LC Change to fishnet

*Explain aggregating landcover change to fishnet*

```{r}
changePoints <-
  rasterToPoints(development_change) %>%
  as.data.frame() %>%
  st_as_sf(coords = c("x", "y"), 
           crs = st_crs(charlotteMSA_fishnet))

fishnet <- 
  aggregate(changePoints, 
            charlotteMSA_fishnet, 
            FUN=sum) %>%
  mutate(development_change = ifelse(is.na(layer) == TRUE , 0, 1),
         development_change = as.factor(development_change)) %>%
  dplyr::select(-layer)

```


*plot land cover change*

```{r}
ggplot() +
  geom_sf(data=charlotteMSA %>% 
            st_transform(crs(fishnet))) +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)$x, 
                 y=xyC(fishnet)$y, 
                 colour=development_change)) +
  scale_colour_manual(values = palette2,
                      labels=c("No Change","New Development"),
                      name = "") +
  labs(title = "Land Cover Development Change", subtitle = "As fishnet centroids") +
  theme_void()
```


## 5.2. Aggregating Batch Land Cover Data to the Fishnet

*Explain aggregating land cover data to put in fishnet*

### 5.2.1. Reclassifying land cover

*Explain simplified classifications* 

| Old_Classification             | New_Classification                                  |
|--------------------------------|-----------------------------------------------------|
| Open Space as well as Low, Medium and High Intensity Development | Developed |
| Deciduous, Evergreen, and Mixed Forest |  Forest |
| Pasture/Hay and Cultivated Crops | Farm |
| Woody and Emergent Herbaceous Wetlands | Woodlands |
| Barren Land, Dwarf Scrub, and Grassland/Herbaceous | Other Undeveloped |
| Water | Water |


*Explain new rasters*

```{r, warning = FALSE, message = FALSE}

developed_2011 <- lc_2011_rs  %in% c(21, 22, 23, 24)
forest_2011 <- lc_2011_rs %in% c(41, 42, 42)
farm_2011 <- lc_2011_rs %in% c(81, 82)
wetlands_2011 <- lc_2011_rs %in% c(90, 95) 
otherUndeveloped_2011 <- lc_2011_rs %in% c(52, 71, 31)
water_2011 <- lc_2011_rs == 11

developed_2019 <- lc_2019_rs %in% c(21, 22, 23, 24)
forest_2019 <- lc_2019_rs %in% c(41, 42, 42)
farm_2019 <- lc_2019_rs %in% c(81, 82)
wetlands_2019 <- lc_2019_rs %in% c(90, 95) 
otherUndeveloped_2019 <- lc_2019_rs %in% c(52, 71, 31)
water_2019 <- lc_2019_rs == 11
```


### 5.2.2. Aggregating Rasters to the Fishnet

*Explain creating two lists of rasters, one for each time point*

```{r, warning = FALSE, message = FALSE}

names(developed_2011) <- "developed_2011"
names(forest_2011) <- "forest_2011"
names(farm_2011) <- "farm_2011"
names(wetlands_2011) <- "wetlands_2011"
names(otherUndeveloped_2011) <- "otherUndeveloped_2011"
names(water_2011) <- "water_2011"

names(developed_2019) <- "developed_2019"
names(forest_2019) <- "forest_2019"
names(farm_2019) <- "farm_2019"
names(wetlands_2019) <- "wetlands_2019"
names(otherUndeveloped_2019) <- "otherUndeveloped_2019"
names(water_2019) <- "water_2019"
```


```{r, warning = FALSE, message = FALSE}
rasterList_2011 <- c(developed_2011,
                   forest_2011,
                   farm_2011,
                   wetlands_2011,
                   otherUndeveloped_2011,
                   water_2011)

rasterList_2019 <- c(developed_2019,
                   forest_2019,
                   farm_2019,
                   wetlands_2019,
                   otherUndeveloped_2019,
                   water_2019)
```

*explain*


```{r, warning = FALSE, message = FALSE}
lcRasters_2011 <-
  aggregateRaster(rasterList_2011, 
                  charlotteMSA_fishnet) %>%
  dplyr::select(developed_2011,
                   forest_2011,
                   farm_2011,
                   wetlands_2011,
                   otherUndeveloped_2011,
                   water_2011) %>%
  mutate_if(is.numeric,as.factor)

lcRasters_2019 <-
  aggregateRaster(rasterList_2019, 
                  charlotteMSA_fishnet) %>%
  dplyr::select(developed_2019,
                   forest_2019,
                   farm_2019,
                   wetlands_2019,
                   otherUndeveloped_2019,
                   water_2019) %>%
  mutate_if(is.numeric,as.factor)

```


*Plot aggregated rasters*

```{r, message = FALSE, warning = FALSE}
lcRasters_2011 %>%
  st_centroid() %>%
 gather(key = "variable", value = "value", developed_2011:water_2011) %>% 
  mutate(X = xyC(.)$x,
         Y = xyC(.)$y) %>%
  ggplot() +
    geom_sf(data=charlotteMSA) +
    geom_point(aes(X,Y, colour=as.factor(value))) +
    facet_wrap(~variable) +
    scale_colour_manual(values = palette2,
                        labels=c("Other","Land Cover"),
                        name = "") +
    labs(title = "Land Cover Types, 2011",
         subtitle = "As fishnet centroids") +
   theme_void()
```

# 6. Wrangle Census Data and Join To the Fishnet

Population density is obviously a critical demand-side component of predicting `Development_Demand`. Census data for both 2011 and 2019 can be downloaded quickly using the `tidycensus` package. As illustrated below, these data are downloaded at a census tract geography and thus, an approach is needed to reconcile tracts and fishnet geometries. This is accomplished using a technique called areal weighted interpolation.

Keep in mind, when you are choosing your time period of interest for your study, that the ACS came into being in the mid 2000s - so models based on data further back might need to use decennial census data sets and different `tidycensus` API calls.

One useful variable that we do not explore in this exercise is population change - it stands to reason that an undeveloped grid cell in or proximate to an area with accelerating growth (e.g. a density higher than that of 5 or 10 years previous) would be likely to convert.

Recall, you will need a census API key to download the census data which must be input with `census_api_key`. 

## 6.1. Downloading Census Data via API

Load your census API key by pasting it into the code block below with the function `census_api_key`

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("e79f3706b6d61249968c6ce88794f6f556e5bf3d", overwrite = TRUE)
```

First data we pull population data for `t1` and reproject it to the same crs as our study fishnet - `charlotteMSA_fishnet`.

In your growth model - you can pull more demographic and economic variables here by adding them to the `acs_vars` list of data that you'd like to call from the Census.

Note that you need to specify your study area in the API call - here I specify the state as `48` (The two-letter abbreviation `TX` would also work), and I specify the county with a list of the counties in my study area.

This is a nice dplyr chain that does the following:

- DL the data, 
- `select` only the Estimates, NAME, and GEOID column (e.g. removing the margin of error columns)
- `rename` my variables to be intelligible, 
- `st_transform` the data to the `charlotteMSA_fishnet` crs
- `st_buffer` the geometries a teeny tiny bit (by negative 1 map unit) - this is done because sometimes (only sometimes) you have twisted and broken geometries, and this fixes it. If you don't do this, when we do aerially weighted interpolation, you might get an error.

```{r, warning = FALSE, message = FALSE, results = "hide"}
# Specify which variable(s) you would like to grab. Here, only one (Total Population) is listed, but you could add more to the call.
acs_vars <- c("B02001_001E")

# Using "tract" as the geography and 2011 as the year, download data data for the Charlotte MSA counties listed.
charlottePop11 <- get_acs(geography = "tract", 
                        variables = acs_vars, 
                        year = 2011,
                        state = 37, 
                        geometry = TRUE, 
                        output = "wide",
                        county=c("Mecklenburg", "Cabarrus", "Gaston", "Union")) %>%
                dplyr::select (GEOID, NAME, acs_vars) %>%
                rename(pop_2011 = B02001_001E) %>%
                st_transform(st_crs(charlotteMSA_fishnet)) %>%
                st_buffer(-1)
```

We do this once more for `t2`, in this case, 2019.


```{r, warning = FALSE, message = FALSE, results = "hide"}
# Specify which variable(s) you would like to grab. Here, only one (Total Population) is listed, but you could add more to the call.
acs_vars <- c("B02001_001E")

# Using "tract" as the geography and 2019 as the year, download data data for the Charlotte MSA counties listed.
charlottePop19 <- get_acs(geography = "tract", 
                        variables = acs_vars, 
                        year = 2019,
                        state = 37, 
                        geometry = TRUE, 
                        output = "wide",
                        county=c("Mecklenburg", "Cabarrus", "Gaston", "Union")) %>%
                dplyr::select (GEOID, NAME, acs_vars) %>%
                rename(pop_2019 = B02001_001E) %>%
                st_transform(st_crs(charlotteMSA_fishnet)) %>%
                st_buffer(-1)
```

Then we can plot our data and see the spatial arrangement of population in our study area in t1 and t2.

One thing worth noting here is that the quantile symbology that we apply probably changes from t1 to t2 - so you can see that, on average, there was densification across the metro area on this time interval - the quantile ranges shift upwards.

<div class="superbigimage">
```{r, warning = FALSE, message = FALSE, fig.height= 8, fig.width= 11}
grid.arrange(
ggplot() +
  geom_sf(data = charlottePop11, aes(fill=factor(ntile(pop_2011,5))), colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=quintileBreaks(charlottePop11,"pop_2011"),
                   name="Quintile\nBreaks") +
  labs(title="Population, Charlotte MSA: 2011") +
  theme_void(),

ggplot() +
  geom_sf(data = charlottePop19, aes(fill=factor(ntile(pop_2019,5))), colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=quintileBreaks(charlottePop19,"pop_2019"),
                   name="Quintile\nBreaks") +
  labs(title="Population, Charlotte MSA: 2019") +
  theme_void(), ncol=2)
```

## 6.2. Aggregate Census Data with Aerially Weighted Interpolation

AWI is a cool spatial analysis tool, you may not have used it before - we can use it to reconcile tract boundaries and fishnet grid cells, and apportion population from the tracts to the cells. Here is how Prof. Steif described AWI -

*Areal weighted interpolation is a really strong spatial analysis skill to have. You can do this in ArcGIS but there is not automated approach.*

*A spatial join would be inappropriate as it would assign the same population value from one tract to the many intersecting grid cells. Instead, the area weighted interpolation function ... assigns a proportion of a tract’s population to a grid cell weighted by the proportion of the tract that intersects the grid cell. This works best of course, when we assume that the tract population is uniformly distributed across the tract. This is typically not a great assumption. However, it is a reasonable one here particularly given population is a feature in a regression and not an outcome that needs to be measured with significant precision. *

*Ken Steif, 2019*

The code chunks below perform AWI for our `t1` and `t2` census data using the `st_interpolate_aw` function. This process might take a bit of time to run. The output is a new fishnet for each year that consists of a population estimate and a geometry. It is important to note that your tracts and your fishnet might have points where they fail to overlap, and you might lose the fidelity of your analysis by dropping fishnet cells. This workflow is 

Here is a breakdown of the workflow so that you can adapt it to use other data sets in your analysis.

- `st_interpolate_aw` takes an sf object (`charlottePop11`), a column name from that sf object (`["pop_2011"]`) and our fishnet `charlotteMSA_fishnet`. Leave the argument `extensive=TRUE` as-is.

- Knowing that our result might have a differing number of cells from the fishnet, we convert this object to centroids (`st_centroid`) and spatially join it to our fishnet using `st_join` - a join initiated from the `charlotteMSA_fishnet` object. This finds the points where our AWI results intersect the fishnet cells, and keeps any cells where there is no intersection,

- Lastly, we perform some data cleaning, where we replace NA values (`replace_na`) with zeros, and `select` only our `pop_2011` column to remain.

Keep an eye on how many cells your fishnets have throughout the process - if you are adding or dropping cells, something is going wrong.

If you get warning messages about "constant or uniform" geometries - this is just the `sf` package reminding you to be on your p's and q's about projections - it won't protect you if your projections don't match!

```{r, warning = FALSE, message = FALSE}
pop_interp <- st_interpolate_aw(charlottePop11["pop_2011"], 
                                 charlotteMSA_fishnet, 
                                 extensive = TRUE)

pop_vec <- rep(NA_real_, nrow(charlotteMSA_fishnet))
pop_vec[match(st_geometry(pop_interp), st_geometry(charlotteMSA_fishnet))] <- pop_interp$pop_2011

fishnetPopulation11 <- charlotteMSA_fishnet %>%
  mutate(pop_2011 = replace_na(pop_vec, 0))
```

We repeat the process for our `t2` data.


```{r}
pop_interp <- st_interpolate_aw(charlottePop19["pop_2019"], 
                                 charlotteMSA_fishnet, 
                                 extensive = TRUE)

pop_vec <- rep(NA_real_, nrow(charlotteMSA_fishnet))
pop_vec[match(st_geometry(pop_interp), st_geometry(charlotteMSA_fishnet))] <- pop_interp$pop_2019

fishnetPopulation19 <- charlotteMSA_fishnet %>%
  mutate(pop_2019 = replace_na(pop_vec, 0))
```

How do these data look when we map them out and compare them to the census vectors? We can see that the modifiable aerial unit issue - where smaller tracts are higher density because of the logic of tract-drawing - no longer distorts the picture - we see increased density in the center, where we expect it.

<div class="superbigimage">
```{r, warning = FALSE, message = FALSE, fig.height = 8, fig.width= 11}
grid.arrange(
ggplot() +
  geom_sf(data=charlottePop11, aes(fill=factor(ntile(pop_2011,5))),colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=substr(quintileBreaks(charlottePop11,"pop_2011"),1,4),
                   name="Quintile\nBreaks") +
  labs(title="Population, Charlotte MSA: 2011",
       subtitle="Represented as tracts; Boundaries omitted") +
  theme_void(),

  ggplot() +
  geom_sf(data=fishnetPopulation11, 
         aes(fill=factor(ntile(pop_2011,5))),colour=NA) +
  scale_fill_manual(values = palette5,
                   labels=substr(quintileBreaks(fishnetPopulation11,"pop_2011"),1,4),
                   name="Quintile\nBreaks") +
  labs(title="Population, Charlotte MSA: 2011",
       subtitle="Represented as fishnet gridcells; Boundaries omitted") +
  theme_void(), ncol=2)
```
</div>

# 7. Transportation And Infrastructure

Accessibility is a key determinant of development potential. The hypothesis here is that the closer a cell is to highway infrastructure, the more likely it is to develop. Accessibility features are engineered by measuring distance from each grid cell to its nearest highway. 

This example only uses highway distance, but one could easily add spatial information about transit lines, bus networks or other relevant infrastructure as predictors. In urban growth modeling, one might model scenarios where new infrastructure is developed, such as a new highway or a new transit line.

To manipulate this workflow and include new infrastructure, you can use ArcGIS or other software to draw a new highway, and then bring that shape into a modeling workflow in this section and calculate new (hypothetical) distance-to-highway variables for each cell in `t2` to use as a basis for forecasting an alternate future `t3` where some cells now have a different level of access to transportation.

## 7.1. Download Highways

First highway vectors (`primary_roads`) are downloaded from the `Tigris` package - we project the data and subset it to the study area using `st_intersection`.

```{r, warning = FALSE, message = FALSE, results = "hide"}
charlotteHighways <-
  tigris::primary_secondary_roads(state = "NC") %>%
  st_transform(st_crs(charlotteMSA)) %>%
  st_intersection(charlotteMSA) %>%
  st_transform(st_crs(fishnet))
```

Let's make a map and examine the spatial relationship between highways and development.

```{r plot_highway, warning = FALSE, message= FALSE}
ggplot() +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)[,1], y=xyC(fishnet)[,2],colour=development_change),size=1.5) +
  geom_sf(data=charlotteHighways) +
  scale_colour_manual(values = palette2,
                      labels=c("No Change","New Development")) +
  labs(title = "New Development and Highways",
       subtitle = "As fishnet centroids") +
  theme_void()
```

## 7.2. Calcuate Distance to Highways

We can create a new fishnet variable called `distance_highways_2011` by turning our `charlotteMSA_fishnet` into centroid points and calculating the distance from each cell centroid to the nearest highway segment. This is done in a `mutate` command that utilizes the `st_distance` function to run this distance calculation for each observation.

We join these observations back to a fishnet, and the result is a grid called `highwayPoints_fishnet` with a column called `distance_highways_2011`.

An important note:

This analysis operates on the *assumption* that the transporation network in `t2` (2019) is the same as in `t1` - so we create a fishnet representing `t2` that is the same - we merely copy it.

*If you are trying to input a different set of transportation infrastructure as the basis for your future projection for t3 - you will want to import this new network info at this step and create an alternative `highwayPoints_fishnet_2019` (or whatever name represents t2 in your analysis). This information will be an important variable in your model that you use to project future development change.*

```{r, warning = FALSE, message = FALSE}
highwayPoints_fishnet_2011 <- charlotteMSA_fishnet %>%
  st_centroid() %>%
  mutate(distance_highways_2011 = as.numeric(st_distance(., st_union(charlotteHighways) %>% 
                                                      st_transform(st_crs(charlotteMSA_fishnet))))) %>%
  as.data.frame() %>% 
  dplyr::select(-geometry) %>% 
  left_join(charlotteMSA_fishnet, .) %>% 
  st_as_sf()

highwayPoints_fishnet_2019 <- highwayPoints_fishnet_2011 %>%
  rename(distance_highways_2019 = distance_highways_2011)

```

```{r, warning = FALSE, message = FALSE}

ggplot() +
  geom_sf(data=charlotteMSA %>% st_transform(st_crs(highwayPoints_fishnet_2011))) +
  geom_point(data=highwayPoints_fishnet_2011, aes(x=xyC(highwayPoints_fishnet_2011)[,1], 
                                             y=xyC(highwayPoints_fishnet_2011)[,2], 
                 colour=factor(ntile(distance_highways_2011,5))),size=1.5) +
  scale_colour_manual(values = palette5,
                      labels=substr(quintileBreaks(highwayPoints_fishnet_2011,"distance_highways_2011"),1,8),
                      name="Quintile\nBreaks") +
  geom_sf(data=charlotteHighways, colour = "red") +
  labs(title = "Distance to Highways (m)",
       subtitle = "As fishnet centroids; Highways visualized in red") +
  theme_void()
```

# 8. Calculate spatial lag of development

As we read in Prof. Steif's introductory essay, the core of our modelling approach is the harnassing of spatial endogenaeity - we hypothesize that the likelihood of a cell to develop is (in part) a function of its proximity to existing development. How do we "parameterize" the adjacency or access of each cell relative to existing development? We use "spatial lag" variables like k-nearest-neighbors (knn).

In this example, we calculate the average distance to each grid cell's 2 nearest developed grid cells in year `t1`. We do this using a custom function that we loaded earlier called `nn_function` to create new variables in our `fishnet` grid for `lagDevelopment_2011` and `lagDevelopment_2019`. 

The `nn_function` takes three arguments - two data frames with lists of x-y coordinates, and a k.

- The first parameter specifies an sf object whose coordinates we want to `measureFrom`, in this case, all `fishnet` centroids. Since the function requires a data frame with x-y coordinates, we take our fishnet, convert it to centroids (`st_centroid`), extract the coordinates to a new data frame (`st_coordinates`, `as.data.frame()`)

- The second, indicates the sf point data we wish to `measureTo`, in this case, the fishnet centroids that were developed in `t1`. We `filter` our `lcRasters_2011` to include only our developed cells, and then we follow the steps we took for our fishnet data, converting to centroid, and extracting coordinates to a new data frame.

- The last parameter is our k - how many neighbors to which we'd like to measure average distance. Why `k=2`? As `k` fluctuates, so does the hypothesized scale of accessibility. One can test the effect of different k parameters on model goodness of fit. A more sophisticated model would hypothesize that this scale can vary significantly from city to suburb to rural town.

```{r, warning = FALSE, message = FALSE}
fishnet$lagDevelopment_2011 <-
    nn_function(fishnet %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                lcRasters_2011 %>%
                  filter(developed_2011 == 1) %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                2)

fishnet$lagDevelopment_2019 <-
    nn_function(fishnet %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                lcRasters_2019 %>%
                  filter(developed_2019 == 1) %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                2)
```

```{r, warning = FALSE, message = FALSE}
ggplot() +
  geom_sf(data=charlotteMSA %>% st_transform(st_crs(fishnet))) +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)[,1], y=xyC(fishnet)[,2], 
                 colour=factor(ntile(lagDevelopment_2011,5))), size=1.5) +
  scale_colour_manual(values = palette5,
                     labels=substr(quintileBreaks(fishnet,"lagDevelopment_2011"),1,7),
                     name="Quintile\nBreaks") +
  labs(title = "Spatial Lag to 2011 Development, m",
       subtitle = "As fishnet centroids") +
  theme_void()
```


# 9. Political Boundaries

We are going to add information about county boundaries to our data set. This serves two important purposes. First, it might be useful to include municipality as a fixed effect in your models to add information about regulatory "culture" or desirability of different areas. Using a geographic fixed effect adds the "average effect" of a particular area on the dependent variable - commonly this is used as a variable in housing price models to incorporate place-based price signals from things like school district or neighborhood reputation. Second, we can use this information to evaluate and analyze our model outputs - seeing where development is expected to take place and evaluate our errors across space.

We can get data on counties from the US Census Bureau via the `tigris` package, transform it to the CRS of our study area, and then `filter` our county data to contain only the relevant shapes for our study area.

```{r, warning = FALSE, message = FALSE, results = "hide"}
options(tigris_class = "sf")

counties <- counties(state = "NC") %>%
  st_as_sf() %>%
  st_transform(st_crs(charlotteMSA_fishnet))

studyAreaCounties <- counties %>%
  filter(NAME %in% c("Mecklenburg", "Cabarrus", "Gaston", "Union"))

```

We can spatially join the `studyAreaCounties` object to our fishnet and create a `countyFishnet` where each cell is imparted with the name of the county it belongs to.

```{r}
countyFishnet <- charlotteMSA_fishnet %>%
  st_join(., studyAreaCounties %>%
            dplyr::select(NAME)) %>%
  as.data.frame() %>% 
  dplyr::select(uniqueID, NAME) %>% 
  left_join(charlotteMSA_fishnet, .) %>% 
  st_as_sf() %>%
  group_by(uniqueID) %>%
  slice(1) %>%
  ungroup() %>%
  arrange(as.numeric(uniqueID))
  
```

# 10. Creating Our Data Set for Modeling

We've done a substantial amount of data wrangling and transformation, and now we can put all of our fishnets together into a data set that we can explore (as part of our feature engineering) and ultimately model. We will need to prepare data sets for `t1` (used to train a model that can predict `development_change` between `t1` and `t2`), and for `t2` (which uses the same variables to forecast change in `t3`).

If there are additional elements that you want to add to your modeling, this is the step where you can put them together. If you are doing your data wrangling using ArcGIS, this is a point at which you'd want to bring your data in to create data sets that are similar to those we create below:

Let's take stock of what we have:

For t1

- fishnet
- highwayPoints_fishnet_2011
- fishnetPopulation11
- lcRasters_2011
- countyFishnet

For t2

- fishnet
- highwayPoints_fishnet_2011
- fishnetPopulation11
- lcRasters_2011
- countyFishnet

Each of these fishnets should have the same number of cells (in this analysis, that's 32816), and given that they were created in the same manner, they will be indexed identically, and can be appended to one another using `cbind`.  Using `select`, we keep only our relevant independent variables (lc type, transport distance, county name, lag to development etc.,), our dependent variable (development change), our uniqueID and our geometry.

In the last two step of our data binding process below we use a function called `rename_with` to remove the string indicating the year from our column names. Why do we do this? It's because our trained model (built using `t1` data) will columns of the same name and data type for `t2` to make predictions. The training data is all it "knows" - so we must prepare similarly formatted data to deploy it.

```{r}


dat_2011 <- 
  cbind( fishnet, highwayPoints_fishnet_2011, fishnetPopulation11, lcRasters_2011, countyFishnet) %>%
  as.data.frame() %>%
  dplyr::select(uniqueID, development_change, lagDevelopment_2011, distance_highways_2011, pop_2011,  
                developed_2011, forest_2011, farm_2011, wetlands_2011, otherUndeveloped_2011, water_2011,
                NAME, geometry) %>%
  filter(water_2011 == 0) %>%
  rename_with(~ str_remove(.x, "_2011"))

dat_2019 <- 
  cbind( fishnet, highwayPoints_fishnet_2019, fishnetPopulation19, lcRasters_2019, countyFishnet) %>%
  as.data.frame() %>%
  dplyr::select(uniqueID, development_change, lagDevelopment_2019, distance_highways_2019, pop_2019,  
                developed_2019, forest_2019, farm_2019, wetlands_2019, otherUndeveloped_2019, water_2019,
                NAME, geometry) %>%
  filter(water_2019 == 0)  %>%
  rename_with(~ str_remove(.x, "_2019"))
  

```

# 11. Feature Exploration

In this section we explore the extent to our possible predictors are associated with development change. If the goal was to predict a continuous variable, scatterplots and correlation coefficients make this process straightforward and relatively easy to explain to a non-technical decison maker.

In this case however, the dependent variable is a binary outcome - either a grid cell was developed between `t1` and `t2` or it wasn’t. In this case, the relevant question is whether for a given feature, there is a notable significant difference in its central tendancies or distribution between areas that changed and areas that did not. These differences are explored in a set of plots below. For models with lots of features, these plots could be complimented by a series of statistical tests examining the difference between a predictor's mean for `1` and `0` cases.

Here, we `select` our continuous varialbes and our dependent variable, convert our to long form using `gather` created facetted bar plots. Note that `geom_bar` calculates the `mean` has a `fun.y` argument where we can calculate the mean value for these variables inside our plotting function. 

How do you interpret each of these plots? Try the following framework - "On average, a cell that converted from undeveloped to developed had was __________ compared to a cell that didn't convert."

Do you think these are going to be good predictors of `development_change`? Why?

```{r, warning = FALSE, message = FALSE}
dat_2011 %>%
  dplyr::select(distance_highways,lagDevelopment,development_change, pop) %>%
  gather(Variable, Value, -development_change) %>%
  ggplot(., aes(development_change, Value, fill=development_change)) + 
    geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
    facet_wrap(~Variable, scales = "free") +
    scale_fill_manual(values = palette2,
                      labels=c("No Change","New Development"),
                      name="Mean Value") +
    labs(title="New Development as a Function of Continuous Variables") +
    theme_minimal() 
```

It's also useful to look at the patterns of change between different land cover types. What proportion of each undeveloped land cover type converted to developed during the t1-t2 interval?


```{r, warning = FALSE, message = FALSE}
dat_2011 %>%
  dplyr::select(development_change, forest, farm, wetlands, otherUndeveloped) %>%
  gather(key = "Land_Cover_Type", Value, -development_change) %>%
     group_by(development_change, Land_Cover_Type) %>%
     summarize(n = sum(as.numeric(Value))) %>%
     ungroup() %>%
    mutate(Conversion_Rate = paste0(round(100 * n/sum(n), 2), "%")) %>%
    filter(development_change == 1) %>%
  dplyr::select(Land_Cover_Type,Conversion_Rate) %>%
  kable() %>% kable_styling(full_width = F)
```

# 12. Modeling

We are going to train and test our models using a workflow based on the one we learned in the ["Introduction to Applied Predictive Modeling" text](https://mafichman.github.io/applied_predictive_modeling/#1_Introduction), where we used Binary Logistic Regression to estimate morbidity in plants.

## 12.1. Splitting our data

First, `dat` is split into 50% training and test sets. Since we have relatively few 1's in our data, a generous train/test split helps keeps a fair number of 1's in the training set. Note how we specify `dat_2011$otherUndeveloped` as needing to be sorted into our training set - there are relatively few of these, and we want to make sure our model has "seen" all levels of our fixed effects in training.

```{r, warning = FALSE, message = FALSE}
set.seed(3456)
trainIndex <- 
  createDataPartition(dat_2011$otherUndeveloped, p = .50,
                                  list = FALSE,
                                  times = 1)
datTrain <- dat_2011[ trainIndex,]
datTest  <- dat_2011[-trainIndex,]

```

## 12.2. Specifying models

We estimate six separate `glm` models - adding new variables for each.

- `Model1` includes only previous land cover types. 

- `Model2` adds the `lagDevelopment`. 

- `Model3` adds population

- `Model4` adds a fixed effect for county (`NAME`)

- `Model5` adds the distance to highway.

- `Model6` is a modification of `Model5` which *interacts* distance to highway and development lag. The hypothesis here is that these two variables are related - the effect of one depends on the other. e.g. Distance to nearest development depends in part on access to transportation. Notice that the effect of both variables is significant in this specification, but both are not significant in `Model5`.

Which model is best? The one that performs the most usefully in our forecasting use case! As it turns out, although county is not significant in our models, it proves useful when we examine our outcomes.

```{r, warning = FALSE, message = FALSE}
Model1 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped, 
              family="binomial"(link="logit"), data = datTrain)

Model2 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + lagDevelopment, 
              family="binomial"(link="logit"), data = datTrain)
              
Model3 <- glm(development_change ~ wetlands + forest  + farm +
                otherUndeveloped + lagDevelopment + pop,
              family="binomial"(link="logit"), data = datTrain)          
              
Model4 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + lagDevelopment + pop + NAME, 
              family="binomial"(link="logit"), data = datTrain) 

Model5 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + lagDevelopment + pop + distance_highways + NAME, 
              family="binomial"(link="logit"), data = datTrain) 

Model6 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + pop + lagDevelopment * distance_highways + NAME, 
              family="binomial"(link="logit"), data = datTrain) 
```


Print the summary objects for your models, and choose one to use as your model for the validation and prediction procedure to follow.


```{r}

summary(Model6)

```
You can compare the AIC indicators for the models to get a sense of their general capacity to explain variation in your dependent variable. The lower the AIC, the better the goodness of fit - remember, this is NOT the same as model "quality" when it comes to prediction!

```{r}
data.frame(
  Model = c("Model1", "Model2", "Model3", "Model4", "Model5", "Model6"),
  AIC = c(Model1$aic, Model2$aic, Model3$aic, Model4$aic, Model5$aic, Model6$aic)
) %>%
  ggplot()+
  geom_bar(aes(x = Model, y = AIC), stat = "identity")+
  theme_minimal()

```

## 12.3. Validating our Model Using the Test Set

We create a new data frame `testSetProbs` that consists of our `class` (e.g. development_change as a 1 or 0), and `probs` - which is the prediction for each observation in our test set using `Model6`. The `type` parameter is set to `response`, which means our `probs` are measures of estimated probability from 0-1.

```{r, warning = FALSE, message = FALSE}
testSetProbs <- 
  data.frame(class = datTest$development_change,
             probs = predict(Model6, datTest, type="response")) 
```

This density plot is a key tool to figure out where we will set our threshold for classifying predicted probabilities as 1's or 0's (eg Predicted to Develop or Predicted Not To Develop).

Take a close look at this plot and see if you have some ideas about what threshold would separate out our 1's from our 0's most effectively with a minimum of error.

```{r}  
ggplot(testSetProbs, aes(probs)) +
  geom_density(aes(fill=class), alpha=0.5) +
  scale_fill_manual(values = palette2,
                    labels=c("No Change","New Development")) +
  labs(title = "Histogram of test set predicted probabilities",
       x="Predicted Probabilities",y="Density") +
  theme_minimal()
```


### 12.3.1. Confusion matrix

Let's say that 0.05 is a good cutoff value - above that value, we predict that cell is a `1`, and below it, a `0`.

Let's build a confusion matrix to see how accurate this model is overall, and what our error rates are.

It helps to write out in plain language what our four outcomes are:

- True Positive: We predicted a cell would convert from undeveloped to developed and it *did* convert.

- False Positive: We predicted a cell would convert from undeveloped to developed and it *did not* convert.

- True Negative: We predicted a cell would *not* convert from undeveloped to developed and it *did not* convert.

- False Negative: We predicted a cell would *not* convert from undeveloped to developed and it *did* convert.

If you want to adjust model performance, or use a different model, you can change the model type in the previous step and/or change the threshold in this step (see the first line of code) and compare the performance.

```{r}
testSetProbs$predClass  = ifelse(testSetProbs$probs > .05 ,1,0)

caret::confusionMatrix(reference = as.factor(testSetProbs$class), 
                       data = as.factor(testSetProbs$predClass), 
                       positive = "1")

```

### 12.3.2. ROC Curve

One more indicator of model performance - the ROC curve. Do we have a "healthy" looking curve? Above the y=x line, not a "hard elbow" shape, and relatively convex?

In this case, we certainly do.

```{r roc_curve, message = FALSE, warning = FALSE}

ggplot(testSetProbs, aes(d = as.numeric(class), m = probs)) + 
  geom_roc(n.cuts = 50, labels = FALSE) + 
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  theme_minimal()
```

## 12.4. Analyzing Error

Now that we have our model, we want to assess our errors across our entire *t2* data set for a few different thresholds and do some "generalizability" tests. 

We should think critically about model thresholds. What does a planner want to optimize for in this situation? Are we mainly concerned with correctly predicting development (True Positives)? What if that means we erroneously predict some areas that *did change* as *no change* (False Negatives)? 

Let's say, for our purposes, that we want to be generous about predicting change, so we have a more general spatial understanding of where change *might* take place, and not accidentally overlook an area where we miss the trend because we were too conservative. You are welcome to take a different approach to this question when you do such a model for yourself - this is a context-dependent choice.

### 12.4.1. Setting a Threshold

Let's compare two different thresholds - 5% and 17%. In plain English, this means that if a predicted probability is greater than 0.05, under the 5% regime, we say "this is classified as a 1".  At a 5% threshold we correctly predict a higher rate of new development areas (True Positive Rate), but incorrectly predicts more no change areas (True Negative Rate).  Conversely, the 17% threshold has a lower True Positive rate and but a far higher True Negative rate. Because most of our cells do not change from *t1* to *t2*, this leads to higher accuracy.

What you think is better here as a threshold? This is a matter of your expertise.

The mechanics of the code below are as follows. 

- We create a new sf object called `dat_2011_preds` starting with `dat_2011` (our whole `t2` data set). 

- We pipe in a mutate command, where we first use the `predict` function to generate probability estimates for each row in `dat_2011` using our model of choice, `Model6`. These estimates are stored in a column called `probs`.

- In a second `mutate` command, we create two new columns classifying our predictions based on `ifelse` statements. e.g. "If `probs` is greater or equal to 0.05, set `Threshold_5_Pct` equal to 1, else set it equal to 0"

- We subsequently use mutate statements to create columns that reflect the accuracy of these classifications using `case_when` statements - which is basically an elaborate `ifelse` statement based on a bunch of conditions - e.g. "If the `Threshold_5_Pct` is a 1, and `development_change` (e.g. the observed), set `confResult_05` to "True_Negative", etc.,

```{r, warning = FALSE, message = FALSE}
dat_2011_preds <-         
  dat_2011 %>%
    mutate(probs = predict(Model6, dat_2011, type="response")) %>%
   mutate(Threshold_5_Pct = as.factor(ifelse(probs >= 0.05 ,1,0)),
           Threshold_17_Pct =  as.factor(ifelse(probs >= 0.17 ,1,0))) %>%
  mutate(confResult_05 =case_when(Threshold_5_Pct == 0 & development_change == 0 ~ "True_Negative",
                              Threshold_5_Pct == 1 & development_change==1 ~ "True_Positive",
                              Threshold_5_Pct == 0 & development_change==1 ~ "False_Negative",
                              Threshold_5_Pct == 1 & development_change ==0 ~ "False_Positive")) %>%
  mutate(confResult_17 =case_when(Threshold_17_Pct == 0 & development_change == 0 ~ "True_Negative",
                              Threshold_17_Pct == 1 & development_change==1 ~ "True_Positive",
                              Threshold_17_Pct == 0 & development_change==1 ~ "False_Negative",
                              Threshold_17_Pct == 1 & development_change ==0 ~ "False_Positive")) %>%
  st_as_sf()
```

Now we can summarize our results by threshold *and* by any other category in our data to see how these thresholds affect our errors.

Let's cross-tabulate our errors by threshold and by County. Do we think there are combinations of county and threshold that are sub-optimal?

```{r}

# Summarize by county and model type
dat_2011_preds %>%
  as.data.frame() %>%
  dplyr::select(confResult_05, confResult_17, NAME) %>%
  pivot_longer(cols = starts_with("confResult"), names_to = "Model_Type", values_to = "Confusion_Result") %>%
  group_by(NAME, Model_Type, Confusion_Result) %>%
  tally() %>%
  pivot_wider(names_from = Confusion_Result, values_from = n, values_fill = 0) %>% # Reshape to wide format
  mutate(TN_Rate_Specificity = 100*( True_Negative/(True_Negative+False_Positive)),
         TP_Rate_Sensitivity = 100*( True_Positive/(True_Positive + False_Negative))) %>%
  dplyr::select(NAME, Model_Type, TN_Rate_Specificity, TP_Rate_Sensitivity) %>%
  kable() %>%
  kable_styling()



```

We can also map our errors and see how our threshold choices manifest themselves across space in our study area.

Notice how the spatial pattern of True Positives for both thresholds is relatively consistent, but the 5% threshold misses most the study area with respect to True Negatives.

<div class="superbigimage">
```{r, warning = FALSE, message= FALSE, fig.height = 6, fig.width= 8}
ggplot() +
  geom_sf(data= dat_2011_preds %>%
            st_centroid() %>%
               dplyr::select(confResult_05, confResult_17, geometry) %>%
               gather(key = "Variable", value = "Value", -geometry), 
             aes(colour=Value)) +
  facet_wrap(~Variable) +
  scale_colour_manual(values = c("red", "yellow", "blue", "grey"), labels=c("False Negative","False Positive", "True Negative", "True Positive"),
                      name="") +
  labs(title="Development Predictions - By Threshold") + 
  theme_void()
```
</div>


# 13. Making A Forecast

OK, we are ready to use our model to forecast future development. Let's use our model `Model6` with a threshold of 0.17 - this will allow us to make a forecast that isn't quite as conservative and minimizes false negatives.

Let's create an sf object called `dat_2027_preds` by feeding our `t2` data to `Model6`.

*To create a forecast that incorporates new transportation infrastructure - your dat_2019 should have distance to transportation that was calculated using a shape that has a new highway as part of its geometry.*

```{r}
dat_2027_preds <- dat_2019 %>%
    mutate(probs = predict(Model6, dat_2019, type="response") ,
           Prediction = as.factor(ifelse(probs >= 0.17 ,1,0))) %>%
  st_as_sf()
  

```


Now lets map it - where are the cells that are classified as likely to develop by 2027?


```{r, warning = FALSE, message = FALSE, fig.height= 8, fig.width= 8 }
ggplot(data=dat_2027_preds) +
  geom_point(aes(x=xyC(dat_2027_preds)[,1], 
                 y=xyC(dat_2027_preds)[,2], colour = Prediction)) +
  geom_sf(data = studyAreaCounties, fill = "transparent")+
  labs(title="Development Predictions, 2027") + 
  theme_void()
```

# 14. Assess Impact

Once we have a forecast, it's the planner's job to think about how to react to the likely impacts of this anticipated demand with a strategy for allocating development through the use of planning - incentives, regulations etc.,

There is a lot of exploratory analysis you might want to do with your model outputs. For example, you could use the `mapview` package to make a quick interactive map and explore communities in your study area in more detail. You can spatially cross-reference your prediction with data found in supplementary data sets, like parks, parcels and other land use data.

## 14.1. Impact Assessment

How much land is anticipated to convert by 2027? How much in total? How much by county? How much sensitive land is expected to convert?

Overall land forecasted land conversion can be converted from cells to raw area by multiplying by the cell resolution - `res(lc_2019_rs)[1]`

```{r}
dat_2027_preds %>%
  as.data.frame() %>%
  filter(Prediction == 1) %>%
  tally() %>%
  rename(total_cells = n) %>%
  mutate(total_area_m = total_cells * res(lc_2019_rs)[1],
         total_km2 = total_area_m/1000000,
         total_mi2 = total_km2*0.386102) %>%
  kable() %>%
  kable_styling ()

```

County-by-county forecast:

```{r}
dat_2027_preds %>%
  as.data.frame() %>%
  filter(Prediction == 1) %>%
  group_by(NAME) %>%
  tally() %>%
  rename(total_cells = n) %>%
  mutate(total_area_m = total_cells * res(lc_2019_rs)[1],
         total_km2 = total_area_m/1000000,
         total_mi2 = total_km2*0.386102) %>%
  kable() %>%
  kable_styling ()

```

Forecast by `t2` land cover type:

```{r}
dat_2027_preds %>%
  as.data.frame() %>%
  filter(Prediction == 1) %>%
  dplyr::select(farm, otherUndeveloped, forest, wetlands, NAME) %>%
  gather(-NAME, key = "Variable", value = "Value") %>%
  group_by(NAME, Variable) %>%
  summarize(total_cells = sum(as.numeric(Value))) %>%
  mutate(total_area_m = total_cells * res(lc_2019_rs)[1],
         total_km2 = total_area_m/1000000,
         total_mi2 = total_km2*0.386102) %>%
  kable() %>%
  kable_styling ()

```

# 15. Next Steps - Towards an Allocation Strategy

What do you do after you have a geographically specific forecast of likely development? You can integrate it with population projection information to see if forecasted growth areas seem adequate to accommodate future growth. You can dive in on areas where sensitive lands are at risk and determine if upzoning in other areas of nearby development might accommodate that population. You can compare future scenarios for fragmentation effects. There are myriad possibilities.

