---
title: "Urban Growth Forecast: Charlotte, NC (CPLN 6750 Final Project, Spring 2025)"
author: "Abe Doroshow & Austin Studner-Sutherland"
date: "5/10/2025"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }


</style>

# 1. Introduction and Planning Rationale

This analysis adopts a reduced-form urban growth modeling approach, inspired by the California Urban Futures Model and adapted through the lens of CPLN 6750. We forecast development in the Charlotte MSA using a logistic regression model trained on raster data. Our goal is to understand the influence of infrastructural accessibility, land cover, and population dynamics on the spatial distribution of future development. We hypothesize that development occurs where higher and better land use is expected to yield returns, moderated by factors like proximity to infrastructure and population pressures.

Charlotte, North Carolina, is one of the fastest-growing metro areas in the southeastern United States. Its population grew by 35.2% from 2000 to 2010, 19.6% from 2010 to 2020, and is forecasted to grow an additional 14.1% from 2020 to 2030. This project aims to forecast urban development in the Charlotte Metropolitan Area, specifically Mecklenburg, Cabarrus, Gaston, and Union counties, by the year 2031. **This scenario includes a proposed new highway running from southeastern Mecklenburg into downtown Charlotte, designed to increase connectivity and redirect growth pressure.**


# 2. Analytical Workflow

This workflow closely mirrors the original instructional steps and includes:

1. **Data Import**: Land cover data for 2011 (t1) and 2019 (t2) are loaded and reclassified to identify developed/undeveloped areas.
2. **Resampling**: The raster data are resampled to a lower resolution to optimize computation.
3. **Land Cover Change Detection**: Using map algebra, we identify grid cells that transitioned from undeveloped to developed.
4. **Fishnet Creation**: A uniform spatial grid (fishnet) overlays the study area for tabular modeling.
5. **Feature Engineering**: Variables include land cover type, population density (via areal weighted interpolation), distance to highways, spatial lag of development, and proximity to planned infrastructure.
6. **Data Integration**: Variables are spatially joined to the fishnet and prepared for modeling.
7. **Exploratory Analysis**: Visualization of features to examine their distribution and potential relationship to development.
8. **Modeling**: A binomial logistic regression is trained to predict land cover change between t1 and t2.
9. **Validation**: Model is validated using a test set, ROC curve, and confusion matrix. Classification thresholds are tested for generalizability.
10. **Forecasting**: t2 data (plus new infrastructure) are input to the model to generate a 2027 (t3) development forecast.
11. **Impact Assessment**: Predicted changes are analyzed in aggregate and by subregion and land type.
12. **Planning Recommendations**: Based on outputs, strategies are proposed to align infrastructure, zoning, and conservation policies.


# 3. Exploratory Analysis and Feature Engineering

We assume a new primairy highway is introduced in southeast Mecklenburg County to enhance commuting access and connection to the city. This infrastructure investment is expected to increase the likelihood of development in surrounding areas. Distance to this corridor is incorporated as a predictor variable in both the t2 model and the t3 forecast.

## 3.1. Setup Libraries, Functions, and Color Pallette

**Load Libraries and Color Pallette:**

```{r load_packages, message=FALSE, warning=FALSE, results = "hide"}
library(tidyverse)
library(sf)
library(raster)
library(kableExtra)
library(tidycensus)
library(tigris)
library(FNN)
library(caret)
library(yardstick)
library(plotROC) 
library(ggrepel)
library(pROC)
library(grid)
library(gridExtra)
library(viridis)
library(igraph)
library(mapview)
library(FedData)

palette2 <- c("#41b6c4","#253494")
palette4 <- c("#a1dab4","#41b6c4","#2c7fb8","#253494")
palette5 <- c("#ffffcc","#a1dab4","#41b6c4","#2c7fb8","#253494")
palette10 <- c("#f7fcf0","#e0f3db","#ccebc5","#a8ddb5","#7bccc4",
               "#4eb3d3","#2b8cbe","#0868ac","#084081","#f7fcf0")
```

**Load functions:** 

```{r, warning = FALSE, message = FALSE}
#this function converts a column in to quintiles. It is used for mapping.
quintileBreaks <- function(df,variable) {
    as.character(quantile(df[[variable]],
                          c(.01,.2,.4,.6,.8),na.rm=T))
}

#This function can be used to convert a polygon sf to centroids xy coords.
xyC <- function(aPolygonSF) {
  as.data.frame(
    cbind(x=st_coordinates(st_centroid(aPolygonSF))[,1],
          y=st_coordinates(st_centroid(aPolygonSF))[,2]))
} 

#this function convert a raster to a data frame so it can be plotted in ggplot
rast <- function(inRaster) {
  data.frame(
    xyFromCell(inRaster, 1:ncell(inRaster)), 
    value = getValues(inRaster)) }

# knn distance function

nn_function <- function(measureFrom,measureTo,k) {
  #convert the sf layers to matrices
  measureFrom_Matrix <-
    as.matrix(measureFrom)
  measureTo_Matrix <-
    as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
    output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull()
  
  return(output)  
}

aggregateRaster <- function(inputRasterList, theFishnet) {
  #create an empty fishnet with the same dimensions as the input fishnet
  theseFishnets <- theFishnet %>% dplyr::select()
  #for each raster in the raster list
  for (i in inputRasterList) {
  #create a variable name corresponding to the ith raster
  varName <- names(i)
  #convert raster to points as an sf
    thesePoints <-
      rasterToPoints(i) %>%
      as.data.frame() %>%
      st_as_sf(coords = c("x", "y"), crs = st_crs(theFishnet)) %>%
      filter(.[[1]] == 1)
  #aggregate to the fishnet
    thisFishnet <-
      aggregate(thesePoints, theFishnet, length) %>%
      mutate(!!varName := ifelse(is.na(.[[1]]),0,1))
  #add to the larger fishnet
    theseFishnets <- cbind(theseFishnets,thisFishnet)
  }
  #output all aggregates as one large fishnet
   return(theseFishnets)
  }
```

## 3.2. Setup the Study Area

A shapefile for the study area was engineered in ArcGIS Pro and exported to a GeoJSON and stored in Github for this analysis. 

**Load Administrative Boundaries and Set Coordinate Reference System (CRS):**

```{r}
charlotteMSA <- 
  st_read("https://raw.githubusercontent.com/austinrs25/CPLN-Urban-Growth-Modeling/refs/heads/main/charlotteMSA.geojson") 
charlotteMSA %>% st_transform(2264)

```
## 3.3. Land Cover Analysis and Workflow

To detect land use change between 2011 (t1) and 2019 (t2), we begin by analyzing land cover rasters for the Charlotte MSA. These rasters are processed to highlight locations where land converted from undeveloped to developed. This binary change variable becomes the dependent variable in our logistic regression model.

**Load Land Cover Data and Clip Raster to Study Area:**

```{r}

lc_2011 <- get_nlcd(
    template = charlotteMSA,
    label = "lc_2011",
    year = 2011
  ) %>%
  raster(.)

```

```{r}

lc_2019 <- get_nlcd(
    template = charlotteMSA,
    label = "lc_2019",
    year = 2019
  ) %>%
  raster(.)

```

**Resample Rasters:**

Given the high resolution of NLCD rasters (30 meters), we resample them to coarser grids to improve processing speed and reduce memory demands during modeling. We use the `aggregate` function from the `raster` package with the `modal` function to summarize land cover within larger cells, preserving the dominant land type.

```{r}
lc_2011_rs <- aggregate(lc_2011, fact = 30, fun = "modal")

lc_2019_rs <- aggregate(lc_2019, fact = 30, fun = "modal")

```

**Reclassify Land Cover as Developend and Undeveloped:**

We reclassify the resampled rasters into two categories: developed (NLCD codes 21â€“24) and undeveloped (all other codes). This results in a simplified binary classification of each cell for both t1 and t2. 

```{r}
reclassMatrix <- 
  matrix(c(
    0,12,0,
    12,24,1,
    24,Inf,0),
  ncol=3, byrow=T)
```

```{r, warning = FALSE, message = FALSE}
developed_2011 <- 
  reclassify(lc_2011_rs, reclassMatrix)

developed_2019 <- 
  reclassify(lc_2019_rs, reclassMatrix)

```

**Detect Change from 2011 (t1) to 2018 (t2):**

Change is then detected using map algebra, identifying cells that were undeveloped in t1 and developed in t2. Cells with no change are assigned NA, allowing us to focus modeling efforts on the subset of cells where development did occur.

```{r, warning = FALSE, message = FALSE}

development_change <- developed_2011 + developed_2019

```

**Visualize Development Change:**

We use the `mapview` package to visualize the resulting change raster interactively. This allows for quick inspection of spatial development patterns and supports verification of our reclassification and change detection steps.

Create histogram to illustrate frequency of conversion from undeveloped to developed between 2011 and 2019 where the 1's represent change. O's and 2's represent no change where 0's demarcate land being undeveloped in both periods and 2 demarcate land be developed in both periods - this presumes no land when from developed to undeveloped.

```{r, warning = FALSE, message = FALSE}
hist(development_change)
```

Create a map from raster to illustrate land cover change, first removing 0's (undeveloped lands) and 2's (developed lands), where 1's represent change (from undeveloped to devloped):

```{r, warning = FALSE, message = FALSE}
development_change[development_change != 1] <- NA
```

```{r}
mapView(development_change)

```

## 3.4. Engineering the Fishnet

**Create Fishnet:**

To facilitate modeling and analysis at a consistent spatial scale, we convert raster data to a regular vector grid, commonly referred to as a fishnet. Each cell in this fishnet becomes a unit of observation, with associated attributes describing land cover and other predictors.

```{r, warning = FALSE, message = FALSE}
charlotteMSA_fishnet <- 
  st_make_grid(charlotteMSA %>%
                     st_transform(crs(development_change)),
                 cellsize = res(development_change)[1], 
                 square = TRUE) %>% 
  st_sf() %>% 
  st_intersection(., charlotteMSA %>%
                    dplyr::select(geometry) %>%
                         st_transform(crs(development_change))) %>%
  mutate(uniqueID = rownames(.))
```

**Aggregate Land Cover Change to the Fishnet:**

First, we convert our binary land cover change raster (indicating development from 2011 to 2019) into points. These are then spatially aggregated to our Charlotte MSA fishnet grid. Each grid cell is assigned a value of 1 if development occurred within its bounds, and 0 otherwise. This step transforms spatially continuous raster data into a tabular form suitable for logistic regression.

```{r}
changePoints <-
  rasterToPoints(development_change) %>%
  as.data.frame() %>%
  st_as_sf(coords = c("x", "y"), 
           crs = st_crs(charlotteMSA_fishnet))

fishnet <- 
  aggregate(changePoints, 
            charlotteMSA_fishnet, 
            FUN=sum) %>%
  mutate(development_change = ifelse(is.na(layer) == TRUE , 0, 1),
         development_change = as.factor(development_change)) %>%
  dplyr::select(-layer)

```

A quick map shows development change across the region. By plotting fishnet cell centroids and coloring them by their change status, we visually confirm where growth has occurred over the decade.

```{r}
ggplot() +
  geom_sf(data=charlotteMSA %>% 
            st_transform(crs(fishnet))) +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)$x, 
                 y=xyC(fishnet)$y, 
                 colour=development_change)) +
  scale_colour_manual(values = palette2,
                      labels=c("No Change","New Development"),
                      name = "") +
  labs(title = "Land Cover Development Change", subtitle = "As fishnet centroids") +
  theme_void()
```

**Reclassify and Aggregate Batch Land Cover Data to the Fishnet:**

To enhance model accuracy, we reclassify the full land cover dataset into broader land use categories: developed, forest, farm, wetlands, other undeveloped, and water. This simplification improves interpretability and reduces data sparsity.

| Old_Classification | New_Classification |
|------------------------------------------------------------------|-------------------|
| Open Space as well as Low, Medium and High Intensity Development | Developed |
| Deciduous, Evergreen, and Mixed Forest |  Forest |
| Pasture/Hay and Cultivated Crops | Farm |
| Woody and Emergent Herbaceous Wetlands | Woodlands |
| Barren Land, Dwarf Scrub, and Grassland/Herbaceous | Other Undeveloped |
| Water | Water |

Create New Classification Rasters:
```{r, warning = FALSE, message = FALSE}

developed_2011 <- lc_2011_rs  %in% c(21, 22, 23, 24)
forest_2011 <- lc_2011_rs %in% c(41, 42, 42)
farm_2011 <- lc_2011_rs %in% c(81, 82)
wetlands_2011 <- lc_2011_rs %in% c(90, 95) 
otherUndeveloped_2011 <- lc_2011_rs %in% c(52, 71, 31)
water_2011 <- lc_2011_rs == 11

developed_2019 <- lc_2019_rs %in% c(21, 22, 23, 24)
forest_2019 <- lc_2019_rs %in% c(41, 42, 42)
farm_2019 <- lc_2019_rs %in% c(81, 82)
wetlands_2019 <- lc_2019_rs %in% c(90, 95) 
otherUndeveloped_2019 <- lc_2019_rs %in% c(52, 71, 31)
water_2019 <- lc_2019_rs == 11
```

For both t1 (2011) and t2 (2018), we construct binary raster layers representing the presence of each land use type. These rasters are named accordingly and compiled into lists.

```{r, warning = FALSE, message = FALSE}

names(developed_2011) <- "developed_2011"
names(forest_2011) <- "forest_2011"
names(farm_2011) <- "farm_2011"
names(wetlands_2011) <- "wetlands_2011"
names(otherUndeveloped_2011) <- "otherUndeveloped_2011"
names(water_2011) <- "water_2011"

names(developed_2019) <- "developed_2019"
names(forest_2019) <- "forest_2019"
names(farm_2019) <- "farm_2019"
names(wetlands_2019) <- "wetlands_2019"
names(otherUndeveloped_2019) <- "otherUndeveloped_2019"
names(water_2019) <- "water_2019"
```


```{r, warning = FALSE, message = FALSE}
rasterList_2011 <- c(developed_2011,
                   forest_2011,
                   farm_2011,
                   wetlands_2011,
                   otherUndeveloped_2011,
                   water_2011)

rasterList_2019 <- c(developed_2019,
                   forest_2019,
                   farm_2019,
                   wetlands_2019,
                   otherUndeveloped_2019,
                   water_2019)
```

Using a batch aggregation function (`aggregateRaster`), we spatially join these raster layers to the fishnet. The result is a dataset where each cell is tagged with binary indicators for each land cover category. This process ensures our model includes rich information about the land characteristics of each grid cell at both time points.

```{r, warning = FALSE, message = FALSE}
lcRasters_2011 <-
  aggregateRaster(rasterList_2011, 
                  charlotteMSA_fishnet) %>%
  dplyr::select(developed_2011,
                   forest_2011,
                   farm_2011,
                   wetlands_2011,
                   otherUndeveloped_2011,
                   water_2011) %>%
  mutate_if(is.numeric,as.factor)

lcRasters_2019 <-
  aggregateRaster(rasterList_2019, 
                  charlotteMSA_fishnet) %>%
  dplyr::select(developed_2019,
                   forest_2019,
                   farm_2019,
                   wetlands_2019,
                   otherUndeveloped_2019,
                   water_2019) %>%
  mutate_if(is.numeric,as.factor)

```

Visualization of the aggregated layers confirms successful mapping. Faceted maps by land cover type show the spatial distribution of different land uses in 2011, helping validate classification and detect emerging development patterns.

```{r, message = FALSE, warning = FALSE}
lcRasters_2011 %>%
  st_centroid() %>%
 gather(key = "variable", value = "value", developed_2011:water_2011) %>% 
  mutate(X = xyC(.)$x,
         Y = xyC(.)$y) %>%
  ggplot() +
    geom_sf(data=charlotteMSA) +
    geom_point(aes(X,Y, colour=as.factor(value))) +
    facet_wrap(~variable) +
    scale_colour_manual(values = palette2,
                        labels=c("Other","Land Cover"),
                        name = "") +
    labs(title = "Land Cover Types, 2011",
         subtitle = "As fishnet centroids") +
   theme_void()
```





**Import Census Data and Joining to the Fishnet:**

Population density serves as a critical demand-side factor in forecasting urban growth. In this analysis, we retrieved total population data for 2011 (t1) and 2019 (t2) using the `tidycensus` package. Since the census data are aggregated at the tract level, we implemented a technique known as areal weighted interpolation (AWI) to align this data with our fishnet grid.

AWI apportions tract-level population values to fishnet cells based on the proportion of spatial overlap, providing a refined spatial approximation of population distribution. This step ensures each fishnet cell has a representative population estimate for both t1 and t2, enabling our model to capture population-driven development patterns.

**Downloading and Tidying Census Data:**

We use the Census API to extract population data for the counties in the Charlotte MSA: Mecklenburg, Cabarrus, Gaston, and Union. Each dataset is transformed into the same CRS as the fishnet and lightly buffered to avoid geometry issues during spatial processing.

Load Census Data:
```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("e79f3706b6d61249968c6ce88794f6f556e5bf3d", overwrite = TRUE)
```

Pull population data for `t1` (2011) and reproject to CRS:
```{r, warning = FALSE, message = FALSE, results = "hide"}
# Specify which variable(s) you would like to grab. Here, only one (Total Population) is listed, but you could add more to the call.
acs_vars <- c("B02001_001E")

# Using "tract" as the geography and 2011 as the year, download data data for the Charlotte MSA counties listed.
charlottePop11 <- get_acs(geography = "tract", 
                        variables = acs_vars, 
                        year = 2011,
                        state = 37, 
                        geometry = TRUE, 
                        output = "wide",
                        county=c("Mecklenburg", "Cabarrus", "Gaston", "Union")) %>%
                dplyr::select (GEOID, NAME, acs_vars) %>%
                rename(pop_2011 = B02001_001E) %>%
                st_transform(st_crs(charlotteMSA_fishnet)) %>%
                st_buffer(-1)
```

Pull population data for `t2` (2019) and reproject to CRS:
```{r, warning = FALSE, message = FALSE, results = "hide"}
# Specify which variable(s) you would like to grab. Here, only one (Total Population) is listed, but you could add more to the call.
acs_vars <- c("B02001_001E")

# Using "tract" as the geography and 2019 as the year, download data data for the Charlotte MSA counties listed.
charlottePop19 <- get_acs(geography = "tract", 
                        variables = acs_vars, 
                        year = 2019,
                        state = 37, 
                        geometry = TRUE, 
                        output = "wide",
                        county=c("Mecklenburg", "Cabarrus", "Gaston", "Union")) %>%
                dplyr::select (GEOID, NAME, acs_vars) %>%
                rename(pop_2019 = B02001_001E) %>%
                st_transform(st_crs(charlotteMSA_fishnet)) %>%
                st_buffer(-1)
```

**Create choropleth maps** to show total population distributed by quintiles, comparing population density in both years (2011 and 2019). This upward shift in population desnsity reflects the Charlotte MSAâ€™s ongoing urbanization, particularly within Mecklenburg County and along key suburban corridors.

<div class="superbigimage"> 
```{r, warning = FALSE, message = FALSE, fig.height= 8, fig.width= 11}
grid.arrange(
ggplot() +
  geom_sf(data = charlottePop11, aes(fill=factor(ntile(pop_2011,5))), colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=quintileBreaks(charlottePop11,"pop_2011"),
                   name="Quintile\nBreaks") +
  labs(title="Population, Charlotte MSA: 2011") +
  theme_void(),

ggplot() +
  geom_sf(data = charlottePop19, aes(fill=factor(ntile(pop_2019,5))), colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=quintileBreaks(charlottePop19,"pop_2019"),
                   name="Quintile\nBreaks") +
  labs(title="Population, Charlotte MSA: 2019") +
  theme_void(), ncol=2)
```
</div>


**Perform Areal Weighted Interpolation (AWI):**

We perform AWI for both 2011 (t1) and 2019 (t2), creating population estimates for each grid cell. These interpolated values are joined back to the fishnet, resulting in spatially consistent population features named `pop_2011` and `pop_2019`.

Perform AWI for `t1` (2011):
```{r, warning = FALSE, message = FALSE}
pop_interp <- st_interpolate_aw(charlottePop11["pop_2011"], 
                                 charlotteMSA_fishnet, 
                                 extensive = TRUE)

pop_vec <- rep(NA_real_, nrow(charlotteMSA_fishnet))
pop_vec[match(st_geometry(pop_interp), st_geometry(charlotteMSA_fishnet))] <- pop_interp$pop_2011

fishnetPopulation11 <- charlotteMSA_fishnet %>%
  mutate(pop_2011 = replace_na(pop_vec, 0))
```

Perform AWI for `t2` (2019):
```{r}
pop_interp <- st_interpolate_aw(charlottePop19["pop_2019"], 
                                 charlotteMSA_fishnet, 
                                 extensive = TRUE)

pop_vec <- rep(NA_real_, nrow(charlotteMSA_fishnet))
pop_vec[match(st_geometry(pop_interp), st_geometry(charlotteMSA_fishnet))] <- pop_interp$pop_2019

fishnetPopulation19 <- charlotteMSA_fishnet %>%
  mutate(pop_2019 = replace_na(pop_vec, 0))
```

This transformation addresses the Modifiable Areal Unit Problem (MAUP) inherent in tract-level data and enhances the spatial resolution of our model inputs. When visualized, these population layers reveal spatial densification patterns across the Charlotte region, with a clear concentration of growth in urban and near-suburban areas.

**Map AWI Transformations:**
<div class="superbigimage"> 
```{r, warning = FALSE, message = FALSE, fig.height = 8, fig.width= 11}
grid.arrange(
ggplot() +
  geom_sf(data=charlottePop11, aes(fill=factor(ntile(pop_2011,5))),colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=substr(quintileBreaks(charlottePop11,"pop_2011"),1,4),
                   name="Quintile\nBreaks") +
  labs(title="Population, Charlotte MSA: 2011",
       subtitle="Represented as tracts; Boundaries omitted") +
  theme_void(),

  ggplot() +
  geom_sf(data=fishnetPopulation11, 
         aes(fill=factor(ntile(pop_2011,5))),colour=NA) +
  scale_fill_manual(values = palette5,
                   labels=substr(quintileBreaks(fishnetPopulation11,"pop_2011"),1,4),
                   name="Quintile\nBreaks") +
  labs(title="Population, Charlotte MSA: 2011",
       subtitle="Represented as fishnet gridcells; Boundaries omitted") +
  theme_void(), ncol=2)
```
</div>








# 7. Transportation And Infrastructure

Accessibility is a key determinant of development potential. The hypothesis here is that the closer a cell is to highway infrastructure, the more likely it is to develop. Accessibility features are engineered by measuring distance from each grid cell to its nearest highway. 

This example only uses highway distance, but one could easily add spatial information about transit lines, bus networks or other relevant infrastructure as predictors. In urban growth modeling, one might model scenarios where new infrastructure is developed, such as a new highway or a new transit line.

To manipulate this workflow and include new infrastructure, you can use ArcGIS or other software to draw a new highway, and then bring that shape into a modeling workflow in this section and calculate new (hypothetical) distance-to-highway variables for each cell in `t2` to use as a basis for forecasting an alternate future `t3` where some cells now have a different level of access to transportation.

## 7.1. Download Highways

First highway vectors (`primary_roads`) are downloaded from the `Tigris` package - we project the data and subset it to the study area using `st_intersection`.

```{r, warning = FALSE, message = FALSE, results = "hide"}
charlotteHighways <-
  tigris::primary_secondary_roads(state = "NC") %>%
  st_transform(st_crs(charlotteMSA)) %>%
  st_intersection(charlotteMSA) %>%
  st_transform(st_crs(fishnet))
```

Let's make a map and examine the spatial relationship between highways and development.

```{r plot_highway, warning = FALSE, message= FALSE}
ggplot() +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)[,1], y=xyC(fishnet)[,2],colour=development_change),size=1.5) +
  geom_sf(data=charlotteHighways) +
  scale_colour_manual(values = palette2,
                      labels=c("No Change","New Development")) +
  labs(title = "New Development and Highways",
       subtitle = "As fishnet centroids") +
  theme_void()
```

## 7.2. Calcuate Distance to Highways

We can create a new fishnet variable called `distance_highways_2011` by turning our `charlotteMSA_fishnet` into centroid points and calculating the distance from each cell centroid to the nearest highway segment. This is done in a `mutate` command that utilizes the `st_distance` function to run this distance calculation for each observation.

We join these observations back to a fishnet, and the result is a grid called `highwayPoints_fishnet` with a column called `distance_highways_2011`.

An important note:

This analysis operates on the *assumption* that the transporation network in `t2` (2019) is the same as in `t1` - so we create a fishnet representing `t2` that is the same - we merely copy it.

*If you are trying to input a different set of transportation infrastructure as the basis for your future projection for t3 - you will want to import this new network info at this step and create an alternative `highwayPoints_fishnet_2019` (or whatever name represents t2 in your analysis). This information will be an important variable in your model that you use to project future development change.*

```{r, warning = FALSE, message = FALSE}
highwayPoints_fishnet_2011 <- charlotteMSA_fishnet %>%
  st_centroid() %>%
  mutate(distance_highways_2011 = as.numeric(st_distance(., st_union(charlotteHighways) %>% 
                                                      st_transform(st_crs(charlotteMSA_fishnet))))) %>%
  as.data.frame() %>% 
  dplyr::select(-geometry) %>% 
  left_join(charlotteMSA_fishnet, .) %>% 
  st_as_sf()

highwayPoints_fishnet_2019 <- highwayPoints_fishnet_2011 %>%
  rename(distance_highways_2019 = distance_highways_2011)

```

```{r, warning = FALSE, message = FALSE}

ggplot() +
  geom_sf(data=charlotteMSA %>% st_transform(st_crs(highwayPoints_fishnet_2011))) +
  geom_point(data=highwayPoints_fishnet_2011, aes(x=xyC(highwayPoints_fishnet_2011)[,1], 
                                             y=xyC(highwayPoints_fishnet_2011)[,2], 
                 colour=factor(ntile(distance_highways_2011,5))),size=1.5) +
  scale_colour_manual(values = palette5,
                      labels=substr(quintileBreaks(highwayPoints_fishnet_2011,"distance_highways_2011"),1,8),
                      name="Quintile\nBreaks") +
  geom_sf(data=charlotteHighways, colour = "red") +
  labs(title = "Distance to Highways (m)",
       subtitle = "As fishnet centroids; Highways visualized in red") +
  theme_void()
```

# 8. Calculate spatial lag of development

As we read in Prof. Steif's introductory essay, the core of our modelling approach is the harnassing of spatial endogenaeity - we hypothesize that the likelihood of a cell to develop is (in part) a function of its proximity to existing development. How do we "parameterize" the adjacency or access of each cell relative to existing development? We use "spatial lag" variables like k-nearest-neighbors (knn).

In this example, we calculate the average distance to each grid cell's 2 nearest developed grid cells in year `t1`. We do this using a custom function that we loaded earlier called `nn_function` to create new variables in our `fishnet` grid for `lagDevelopment_2011` and `lagDevelopment_2019`. 

The `nn_function` takes three arguments - two data frames with lists of x-y coordinates, and a k.

- The first parameter specifies an sf object whose coordinates we want to `measureFrom`, in this case, all `fishnet` centroids. Since the function requires a data frame with x-y coordinates, we take our fishnet, convert it to centroids (`st_centroid`), extract the coordinates to a new data frame (`st_coordinates`, `as.data.frame()`)

- The second, indicates the sf point data we wish to `measureTo`, in this case, the fishnet centroids that were developed in `t1`. We `filter` our `lcRasters_2011` to include only our developed cells, and then we follow the steps we took for our fishnet data, converting to centroid, and extracting coordinates to a new data frame.

- The last parameter is our k - how many neighbors to which we'd like to measure average distance. Why `k=2`? As `k` fluctuates, so does the hypothesized scale of accessibility. One can test the effect of different k parameters on model goodness of fit. A more sophisticated model would hypothesize that this scale can vary significantly from city to suburb to rural town.

```{r, warning = FALSE, message = FALSE}
fishnet$lagDevelopment_2011 <-
    nn_function(fishnet %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                lcRasters_2011 %>%
                  filter(developed_2011 == 1) %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                2)

fishnet$lagDevelopment_2019 <-
    nn_function(fishnet %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                lcRasters_2019 %>%
                  filter(developed_2019 == 1) %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                2)
```

```{r, warning = FALSE, message = FALSE}
ggplot() +
  geom_sf(data=charlotteMSA %>% st_transform(st_crs(fishnet))) +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)[,1], y=xyC(fishnet)[,2], 
                 colour=factor(ntile(lagDevelopment_2011,5))), size=1.5) +
  scale_colour_manual(values = palette5,
                     labels=substr(quintileBreaks(fishnet,"lagDevelopment_2011"),1,7),
                     name="Quintile\nBreaks") +
  labs(title = "Spatial Lag to 2011 Development, m",
       subtitle = "As fishnet centroids") +
  theme_void()
```


# 9. Political Boundaries

We are going to add information about county boundaries to our data set. This serves two important purposes. First, it might be useful to include municipality as a fixed effect in your models to add information about regulatory "culture" or desirability of different areas. Using a geographic fixed effect adds the "average effect" of a particular area on the dependent variable - commonly this is used as a variable in housing price models to incorporate place-based price signals from things like school district or neighborhood reputation. Second, we can use this information to evaluate and analyze our model outputs - seeing where development is expected to take place and evaluate our errors across space.

We can get data on counties from the US Census Bureau via the `tigris` package, transform it to the CRS of our study area, and then `filter` our county data to contain only the relevant shapes for our study area.

```{r, warning = FALSE, message = FALSE, results = "hide"}
options(tigris_class = "sf")

counties <- counties(state = "NC") %>%
  st_as_sf() %>%
  st_transform(st_crs(charlotteMSA_fishnet))

studyAreaCounties <- counties %>%
  filter(NAME %in% c("Mecklenburg", "Cabarrus", "Gaston", "Union"))

```

We can spatially join the `studyAreaCounties` object to our fishnet and create a `countyFishnet` where each cell is imparted with the name of the county it belongs to.

```{r}
countyFishnet <- charlotteMSA_fishnet %>%
  st_join(., studyAreaCounties %>%
            dplyr::select(NAME)) %>%
  as.data.frame() %>% 
  dplyr::select(uniqueID, NAME) %>% 
  left_join(charlotteMSA_fishnet, .) %>% 
  st_as_sf() %>%
  group_by(uniqueID) %>%
  slice(1) %>%
  ungroup() %>%
  arrange(as.numeric(uniqueID))
  
```

# 10. Creating Our Data Set for Modeling

We've done a substantial amount of data wrangling and transformation, and now we can put all of our fishnets together into a data set that we can explore (as part of our feature engineering) and ultimately model. We will need to prepare data sets for `t1` (used to train a model that can predict `development_change` between `t1` and `t2`), and for `t2` (which uses the same variables to forecast change in `t3`).

If there are additional elements that you want to add to your modeling, this is the step where you can put them together. If you are doing your data wrangling using ArcGIS, this is a point at which you'd want to bring your data in to create data sets that are similar to those we create below:

Let's take stock of what we have:

For t1

- fishnet
- highwayPoints_fishnet_2011
- fishnetPopulation11
- lcRasters_2011
- countyFishnet

For t2

- fishnet
- highwayPoints_fishnet_2011
- fishnetPopulation11
- lcRasters_2011
- countyFishnet

Each of these fishnets should have the same number of cells (in this analysis, that's 32816), and given that they were created in the same manner, they will be indexed identically, and can be appended to one another using `cbind`.  Using `select`, we keep only our relevant independent variables (lc type, transport distance, county name, lag to development etc.,), our dependent variable (development change), our uniqueID and our geometry.

In the last two step of our data binding process below we use a function called `rename_with` to remove the string indicating the year from our column names. Why do we do this? It's because our trained model (built using `t1` data) will columns of the same name and data type for `t2` to make predictions. The training data is all it "knows" - so we must prepare similarly formatted data to deploy it.

```{r}


dat_2011 <- 
  cbind( fishnet, highwayPoints_fishnet_2011, fishnetPopulation11, lcRasters_2011, countyFishnet) %>%
  as.data.frame() %>%
  dplyr::select(uniqueID, development_change, lagDevelopment_2011, distance_highways_2011, pop_2011,  
                developed_2011, forest_2011, farm_2011, wetlands_2011, otherUndeveloped_2011, water_2011,
                NAME, geometry) %>%
  filter(water_2011 == 0) %>%
  rename_with(~ str_remove(.x, "_2011"))

dat_2019 <- 
  cbind( fishnet, highwayPoints_fishnet_2019, fishnetPopulation19, lcRasters_2019, countyFishnet) %>%
  as.data.frame() %>%
  dplyr::select(uniqueID, development_change, lagDevelopment_2019, distance_highways_2019, pop_2019,  
                developed_2019, forest_2019, farm_2019, wetlands_2019, otherUndeveloped_2019, water_2019,
                NAME, geometry) %>%
  filter(water_2019 == 0)  %>%
  rename_with(~ str_remove(.x, "_2019"))
  

```

# 11. Feature Exploration

In this section we explore the extent to our possible predictors are associated with development change. If the goal was to predict a continuous variable, scatterplots and correlation coefficients make this process straightforward and relatively easy to explain to a non-technical decison maker.

In this case however, the dependent variable is a binary outcome - either a grid cell was developed between `t1` and `t2` or it wasnâ€™t. In this case, the relevant question is whether for a given feature, there is a notable significant difference in its central tendancies or distribution between areas that changed and areas that did not. These differences are explored in a set of plots below. For models with lots of features, these plots could be complimented by a series of statistical tests examining the difference between a predictor's mean for `1` and `0` cases.

Here, we `select` our continuous varialbes and our dependent variable, convert our to long form using `gather` created facetted bar plots. Note that `geom_bar` calculates the `mean` has a `fun.y` argument where we can calculate the mean value for these variables inside our plotting function. 

How do you interpret each of these plots? Try the following framework - "On average, a cell that converted from undeveloped to developed had was __________ compared to a cell that didn't convert."

Do you think these are going to be good predictors of `development_change`? Why?

```{r, warning = FALSE, message = FALSE}
dat_2011 %>%
  dplyr::select(distance_highways,lagDevelopment,development_change, pop) %>%
  gather(Variable, Value, -development_change) %>%
  ggplot(., aes(development_change, Value, fill=development_change)) + 
    geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
    facet_wrap(~Variable, scales = "free") +
    scale_fill_manual(values = palette2,
                      labels=c("No Change","New Development"),
                      name="Mean Value") +
    labs(title="New Development as a Function of Continuous Variables") +
    theme_minimal() 
```

It's also useful to look at the patterns of change between different land cover types. What proportion of each undeveloped land cover type converted to developed during the t1-t2 interval?


```{r, warning = FALSE, message = FALSE}
dat_2011 %>%
  dplyr::select(development_change, forest, farm, wetlands, otherUndeveloped) %>%
  gather(key = "Land_Cover_Type", Value, -development_change) %>%
     group_by(development_change, Land_Cover_Type) %>%
     summarize(n = sum(as.numeric(Value))) %>%
     ungroup() %>%
    mutate(Conversion_Rate = paste0(round(100 * n/sum(n), 2), "%")) %>%
    filter(development_change == 1) %>%
  dplyr::select(Land_Cover_Type,Conversion_Rate) %>%
  kable() %>% kable_styling(full_width = F)
```

# 12. Modeling

We are going to train and test our models using a workflow based on the one we learned in the ["Introduction to Applied Predictive Modeling" text](https://mafichman.github.io/applied_predictive_modeling/#1_Introduction), where we used Binary Logistic Regression to estimate morbidity in plants.

## 12.1. Splitting our data

First, `dat` is split into 50% training and test sets. Since we have relatively few 1's in our data, a generous train/test split helps keeps a fair number of 1's in the training set. Note how we specify `dat_2011$otherUndeveloped` as needing to be sorted into our training set - there are relatively few of these, and we want to make sure our model has "seen" all levels of our fixed effects in training.

```{r, warning = FALSE, message = FALSE}
set.seed(3456)
trainIndex <- 
  createDataPartition(dat_2011$otherUndeveloped, p = .50,
                                  list = FALSE,
                                  times = 1)
datTrain <- dat_2011[ trainIndex,]
datTest  <- dat_2011[-trainIndex,]

```

## 12.2. Specifying models

We estimate six separate `glm` models - adding new variables for each.

- `Model1` includes only previous land cover types. 

- `Model2` adds the `lagDevelopment`. 

- `Model3` adds population

- `Model4` adds a fixed effect for county (`NAME`)

- `Model5` adds the distance to highway.

- `Model6` is a modification of `Model5` which *interacts* distance to highway and development lag. The hypothesis here is that these two variables are related - the effect of one depends on the other. e.g. Distance to nearest development depends in part on access to transportation. Notice that the effect of both variables is significant in this specification, but both are not significant in `Model5`.

Which model is best? The one that performs the most usefully in our forecasting use case! As it turns out, although county is not significant in our models, it proves useful when we examine our outcomes.

```{r, warning = FALSE, message = FALSE}
Model1 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped, 
              family="binomial"(link="logit"), data = datTrain)

Model2 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + lagDevelopment, 
              family="binomial"(link="logit"), data = datTrain)
              
Model3 <- glm(development_change ~ wetlands + forest  + farm +
                otherUndeveloped + lagDevelopment + pop,
              family="binomial"(link="logit"), data = datTrain)          
              
Model4 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + lagDevelopment + pop + NAME, 
              family="binomial"(link="logit"), data = datTrain) 

Model5 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + lagDevelopment + pop + distance_highways + NAME, 
              family="binomial"(link="logit"), data = datTrain) 

Model6 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + pop + lagDevelopment * distance_highways + NAME, 
              family="binomial"(link="logit"), data = datTrain) 
```


Print the summary objects for your models, and choose one to use as your model for the validation and prediction procedure to follow.


```{r}

summary(Model6)

```
You can compare the AIC indicators for the models to get a sense of their general capacity to explain variation in your dependent variable. The lower the AIC, the better the goodness of fit - remember, this is NOT the same as model "quality" when it comes to prediction!

```{r}
data.frame(
  Model = c("Model1", "Model2", "Model3", "Model4", "Model5", "Model6"),
  AIC = c(Model1$aic, Model2$aic, Model3$aic, Model4$aic, Model5$aic, Model6$aic)
) %>%
  ggplot()+
  geom_bar(aes(x = Model, y = AIC), stat = "identity")+
  theme_minimal()

```

## 12.3. Validating our Model Using the Test Set

We create a new data frame `testSetProbs` that consists of our `class` (e.g. development_change as a 1 or 0), and `probs` - which is the prediction for each observation in our test set using `Model6`. The `type` parameter is set to `response`, which means our `probs` are measures of estimated probability from 0-1.

```{r, warning = FALSE, message = FALSE}
testSetProbs <- 
  data.frame(class = datTest$development_change,
             probs = predict(Model6, datTest, type="response")) 
```

This density plot is a key tool to figure out where we will set our threshold for classifying predicted probabilities as 1's or 0's (eg Predicted to Develop or Predicted Not To Develop).

Take a close look at this plot and see if you have some ideas about what threshold would separate out our 1's from our 0's most effectively with a minimum of error.

```{r}  
ggplot(testSetProbs, aes(probs)) +
  geom_density(aes(fill=class), alpha=0.5) +
  scale_fill_manual(values = palette2,
                    labels=c("No Change","New Development")) +
  labs(title = "Histogram of test set predicted probabilities",
       x="Predicted Probabilities",y="Density") +
  theme_minimal()
```


### 12.3.1. Confusion matrix

Let's say that 0.05 is a good cutoff value - above that value, we predict that cell is a `1`, and below it, a `0`.

Let's build a confusion matrix to see how accurate this model is overall, and what our error rates are.

It helps to write out in plain language what our four outcomes are:

- True Positive: We predicted a cell would convert from undeveloped to developed and it *did* convert.

- False Positive: We predicted a cell would convert from undeveloped to developed and it *did not* convert.

- True Negative: We predicted a cell would *not* convert from undeveloped to developed and it *did not* convert.

- False Negative: We predicted a cell would *not* convert from undeveloped to developed and it *did* convert.

If you want to adjust model performance, or use a different model, you can change the model type in the previous step and/or change the threshold in this step (see the first line of code) and compare the performance.

```{r}
testSetProbs$predClass  = ifelse(testSetProbs$probs > .05 ,1,0)

caret::confusionMatrix(reference = as.factor(testSetProbs$class), 
                       data = as.factor(testSetProbs$predClass), 
                       positive = "1")

```

### 12.3.2. ROC Curve

One more indicator of model performance - the ROC curve. Do we have a "healthy" looking curve? Above the y=x line, not a "hard elbow" shape, and relatively convex?

In this case, we certainly do.

```{r roc_curve, message = FALSE, warning = FALSE}

ggplot(testSetProbs, aes(d = as.numeric(class), m = probs)) + 
  geom_roc(n.cuts = 50, labels = FALSE) + 
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  theme_minimal()
```

## 12.4. Analyzing Error

Now that we have our model, we want to assess our errors across our entire *t2* data set for a few different thresholds and do some "generalizability" tests. 

We should think critically about model thresholds. What does a planner want to optimize for in this situation? Are we mainly concerned with correctly predicting development (True Positives)? What if that means we erroneously predict some areas that *did change* as *no change* (False Negatives)? 

Let's say, for our purposes, that we want to be generous about predicting change, so we have a more general spatial understanding of where change *might* take place, and not accidentally overlook an area where we miss the trend because we were too conservative. You are welcome to take a different approach to this question when you do such a model for yourself - this is a context-dependent choice.

### 12.4.1. Setting a Threshold

Let's compare two different thresholds - 5% and 17%. In plain English, this means that if a predicted probability is greater than 0.05, under the 5% regime, we say "this is classified as a 1".  At a 5% threshold we correctly predict a higher rate of new development areas (True Positive Rate), but incorrectly predicts more no change areas (True Negative Rate).  Conversely, the 17% threshold has a lower True Positive rate and but a far higher True Negative rate. Because most of our cells do not change from *t1* to *t2*, this leads to higher accuracy.

What you think is better here as a threshold? This is a matter of your expertise.

The mechanics of the code below are as follows. 

- We create a new sf object called `dat_2011_preds` starting with `dat_2011` (our whole `t2` data set). 

- We pipe in a mutate command, where we first use the `predict` function to generate probability estimates for each row in `dat_2011` using our model of choice, `Model6`. These estimates are stored in a column called `probs`.

- In a second `mutate` command, we create two new columns classifying our predictions based on `ifelse` statements. e.g. "If `probs` is greater or equal to 0.05, set `Threshold_5_Pct` equal to 1, else set it equal to 0"

- We subsequently use mutate statements to create columns that reflect the accuracy of these classifications using `case_when` statements - which is basically an elaborate `ifelse` statement based on a bunch of conditions - e.g. "If the `Threshold_5_Pct` is a 1, and `development_change` (e.g. the observed), set `confResult_05` to "True_Negative", etc.,

```{r, warning = FALSE, message = FALSE}
dat_2011_preds <-         
  dat_2011 %>%
    mutate(probs = predict(Model6, dat_2011, type="response")) %>%
   mutate(Threshold_5_Pct = as.factor(ifelse(probs >= 0.05 ,1,0)),
           Threshold_17_Pct =  as.factor(ifelse(probs >= 0.17 ,1,0))) %>%
  mutate(confResult_05 =case_when(Threshold_5_Pct == 0 & development_change == 0 ~ "True_Negative",
                              Threshold_5_Pct == 1 & development_change==1 ~ "True_Positive",
                              Threshold_5_Pct == 0 & development_change==1 ~ "False_Negative",
                              Threshold_5_Pct == 1 & development_change ==0 ~ "False_Positive")) %>%
  mutate(confResult_17 =case_when(Threshold_17_Pct == 0 & development_change == 0 ~ "True_Negative",
                              Threshold_17_Pct == 1 & development_change==1 ~ "True_Positive",
                              Threshold_17_Pct == 0 & development_change==1 ~ "False_Negative",
                              Threshold_17_Pct == 1 & development_change ==0 ~ "False_Positive")) %>%
  st_as_sf()
```

Now we can summarize our results by threshold *and* by any other category in our data to see how these thresholds affect our errors.

Let's cross-tabulate our errors by threshold and by County. Do we think there are combinations of county and threshold that are sub-optimal?

```{r}

# Summarize by county and model type
dat_2011_preds %>%
  as.data.frame() %>%
  dplyr::select(confResult_05, confResult_17, NAME) %>%
  pivot_longer(cols = starts_with("confResult"), names_to = "Model_Type", values_to = "Confusion_Result") %>%
  group_by(NAME, Model_Type, Confusion_Result) %>%
  tally() %>%
  pivot_wider(names_from = Confusion_Result, values_from = n, values_fill = 0) %>% # Reshape to wide format
  mutate(TN_Rate_Specificity = 100*( True_Negative/(True_Negative+False_Positive)),
         TP_Rate_Sensitivity = 100*( True_Positive/(True_Positive + False_Negative))) %>%
  dplyr::select(NAME, Model_Type, TN_Rate_Specificity, TP_Rate_Sensitivity) %>%
  kable() %>%
  kable_styling()



```

We can also map our errors and see how our threshold choices manifest themselves across space in our study area.

Notice how the spatial pattern of True Positives for both thresholds is relatively consistent, but the 5% threshold misses most the study area with respect to True Negatives.

<div class="superbigimage">
```{r, warning = FALSE, message= FALSE, fig.height = 6, fig.width= 8}
ggplot() +
  geom_sf(data= dat_2011_preds %>%
            st_centroid() %>%
               dplyr::select(confResult_05, confResult_17, geometry) %>%
               gather(key = "Variable", value = "Value", -geometry), 
             aes(colour=Value)) +
  facet_wrap(~Variable) +
  scale_colour_manual(values = c("red", "yellow", "blue", "grey"), labels=c("False Negative","False Positive", "True Negative", "True Positive"),
                      name="") +
  labs(title="Development Predictions - By Threshold") + 
  theme_void()
```
</div>


# 13. Making A Forecast

OK, we are ready to use our model to forecast future development. Let's use our model `Model6` with a threshold of 0.17 - this will allow us to make a forecast that isn't quite as conservative and minimizes false negatives.

Let's create an sf object called `dat_2027_preds` by feeding our `t2` data to `Model6`.

*To create a forecast that incorporates new transportation infrastructure - your dat_2019 should have distance to transportation that was calculated using a shape that has a new highway as part of its geometry.*

```{r}
dat_2027_preds <- dat_2019 %>%
    mutate(probs = predict(Model6, dat_2019, type="response") ,
           Prediction = as.factor(ifelse(probs >= 0.17 ,1,0))) %>%
  st_as_sf()
  

```


Now lets map it - where are the cells that are classified as likely to develop by 2027?


```{r, warning = FALSE, message = FALSE, fig.height= 8, fig.width= 8 }
ggplot(data=dat_2027_preds) +
  geom_point(aes(x=xyC(dat_2027_preds)[,1], 
                 y=xyC(dat_2027_preds)[,2], colour = Prediction)) +
  geom_sf(data = studyAreaCounties, fill = "transparent")+
  labs(title="Development Predictions, 2027") + 
  theme_void()
```

# 14. Assess Impact

Once we have a forecast, it's the planner's job to think about how to react to the likely impacts of this anticipated demand with a strategy for allocating development through the use of planning - incentives, regulations etc.,

There is a lot of exploratory analysis you might want to do with your model outputs. For example, you could use the `mapview` package to make a quick interactive map and explore communities in your study area in more detail. You can spatially cross-reference your prediction with data found in supplementary data sets, like parks, parcels and other land use data.

## 14.1. Impact Assessment

How much land is anticipated to convert by 2027? How much in total? How much by county? How much sensitive land is expected to convert?

Overall land forecasted land conversion can be converted from cells to raw area by multiplying by the cell resolution - `res(lc_2019_rs)[1]`

```{r}
dat_2027_preds %>%
  as.data.frame() %>%
  filter(Prediction == 1) %>%
  tally() %>%
  rename(total_cells = n) %>%
  mutate(total_area_m = total_cells * res(lc_2019_rs)[1],
         total_km2 = total_area_m/1000000,
         total_mi2 = total_km2*0.386102) %>%
  kable() %>%
  kable_styling ()

```

County-by-county forecast:

```{r}
dat_2027_preds %>%
  as.data.frame() %>%
  filter(Prediction == 1) %>%
  group_by(NAME) %>%
  tally() %>%
  rename(total_cells = n) %>%
  mutate(total_area_m = total_cells * res(lc_2019_rs)[1],
         total_km2 = total_area_m/1000000,
         total_mi2 = total_km2*0.386102) %>%
  kable() %>%
  kable_styling ()

```

Forecast by `t2` land cover type:

```{r}
dat_2027_preds %>%
  as.data.frame() %>%
  filter(Prediction == 1) %>%
  dplyr::select(farm, otherUndeveloped, forest, wetlands, NAME) %>%
  gather(-NAME, key = "Variable", value = "Value") %>%
  group_by(NAME, Variable) %>%
  summarize(total_cells = sum(as.numeric(Value))) %>%
  mutate(total_area_m = total_cells * res(lc_2019_rs)[1],
         total_km2 = total_area_m/1000000,
         total_mi2 = total_km2*0.386102) %>%
  kable() %>%
  kable_styling ()

```

# 15. Next Steps - Towards an Allocation Strategy

What do you do after you have a geographically specific forecast of likely development? You can integrate it with population projection information to see if forecasted growth areas seem adequate to accommodate future growth. You can dive in on areas where sensitive lands are at risk and determine if upzoning in other areas of nearby development might accommodate that population. You can compare future scenarios for fragmentation effects. There are myriad possibilities.

# Data Sources

- **Land Cover Data**: Multi-Resolution Land Characteristics (MRLC) Consortium â€“ [https://www.mrlc.gov/data?f%5B0%5D=category%3ALand%20Cover](https://www.mrlc.gov/data?f%5B0%5D=category%3ALand%20Cover)
- **Census Data**: United States Census Bureau â€“ [https://api.census.gov/data/key_signup.html](https://api.census.gov/data/key_signup.html)
- **Roads Data**: Data.gov, accessed using the `Tigris` R package â€“ [https://catalog.data.gov/dataset/tiger-line-shapefile-2023-state-north-carolina-primary-and-secondary-roads](https://catalog.data.gov/dataset/tiger-line-shapefile-2023-state-north-carolina-primary-and-secondary-roads)
- **County Boundaries**: North Carolina Department of Transportation â€“ [https://www.nconemap.gov/datasets/NCDOT::ncdot-county-boundaries/about](https://www.nconemap.gov/datasets/NCDOT::ncdot-county-boundaries/about) how accessible, reproducible spatial models in R can be used to simulate urban futures. The analysis provides a rigorous base for discussing trade-offs, infrastructure needs, and policy levers to guide equitable and sustainable growth in the Charlotte MSA.
